{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, the dataset contains 10,000 books, but we've decided to pick only 3000, to create network of more appropriate size. \n",
    "\n",
    "The reason is, there are 6 milion ratings in total - which results in too dense graph. We still had to remove edges which had weight lower then a certain threshold to get to a managable size of the graph.\n",
    "\n",
    "**After cleaning our dataset is in total around 100MB.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "- In the rest of this notebook we will explain how we prepared our dataset for analysis\n",
    "\n",
    "### 2.1 Tags\n",
    "Goodbooks-10k tags are in two files\n",
    "1. book_tags.csv: \n",
    "    - book_id \n",
    "    - tag_id \n",
    "    - number of users who gave this tag\n",
    "2. tags.csv: \n",
    "    - tag_id \n",
    "    - tag_name\n",
    "    \n",
    "We want to keep only **top 5 tags** per book and we need to transform the data in resulting form:\n",
    "- book_id\n",
    "- tag_name\n",
    "- number of users who gave this tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#load book tags from csv\n",
    "booktags = pd.read_csv(\".\\\\data\\\\book_tags.csv\")\n",
    "#load tag names\n",
    "tagnames = pd.read_csv(\".\\\\data\\\\tags.csv\")\n",
    "#load list of 3000 book ids\n",
    "bookids = pd.read_pickle(\".\\\\data\\\\book_id3000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate tag id to tag name\n",
    "tagdict = tagnames.set_index('tag_id')['tag_name'].to_dict()\n",
    "booktags['tag_name'] = booktags['tag_id'].map(tagdict)\n",
    "booktags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blacklisting tags\n",
    "- we also want to remove frequent tags, which don't tell us information about the book genre, but are used to categorize the users' libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist_strict = [\"to-read\",\"currently-reading\", \"favorites\", \"owned-books\", \"books-i-own\", \"owned\", \"re-read\", \"library\",\n",
    "            \"kindle\", \"default\", \"ebook\", \"my-books\",\"wish-list\",\"my-library\", \"audiobooks\",\"i-own\", \"audio\", \"favourites\", \"own-it\",\n",
    "                \"e-book\", \"e-books\", \"to-buy\", \"audiobook\", \"ebooks\",\"books\", \"audible\",\"audio-books\", \"audio-book\", \"have\"]\n",
    "#remove blacklisted tags\n",
    "for bl in blacklist_strict:\n",
    "    booktags.drop(booktags[booktags.tag_name == bl].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find Top 5 tags for each book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = pd.DataFrame()\n",
    "res = list()\n",
    "\n",
    "#sort booktags\n",
    "booktags.sort_values(by=['count'],ascending=False)\n",
    "\n",
    "#find top 5 tags for each book_id\n",
    "for i in bookids: \n",
    "    bookid = i\n",
    "    BI = booktags[\"goodreads_book_id\"] == bookid\n",
    "    tags = booktags[BI][:5]\n",
    "    res.append(tags)\n",
    "    \n",
    "#concatenate all dataframes into 1\n",
    "for r in res:\n",
    "    top5 = pd.concat([r,top5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Reviews & Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading from GoodReads\n",
    "- our selected dataset doesn't contain user reviews and book description, so we used following script to download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html.clean import clean_html as lxml_clean_html #cleaning javascript from html\n",
    "from bs4 import BeautifulSoup #html tag removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load book ids\n",
    "with open('.\\\\data\\\\book_id3000.pkl', 'rb') as f:\n",
    "    bookIDs = pickle.load(f)\n",
    "\n",
    "baseUrl = \"https://www.goodreads.com/book/show/\"\n",
    "\n",
    "err_list = list()\n",
    "for index, row in df.iterrows():\n",
    "    #take a book id\n",
    "    bookID = df.at[index,'bookID']\n",
    "    #create a query for the book\n",
    "    query = \"%s%s\"%(baseUrl,bookID)\n",
    "    try:\n",
    "        #download html page\n",
    "        res = urlopen(query)\n",
    "    except:\n",
    "        #save bookids with error\n",
    "        err_list.append(bookID)\n",
    "        continue\n",
    "     \n",
    "    #use utf-8 encoding and ignore other characters\n",
    "    source = res.read().decode('utf-8', 'ignore')\n",
    "    \n",
    "    #removes java sctript\n",
    "    source = lxml_clean_html(source)\n",
    "    \n",
    "    #save page\n",
    "    df.at[index,'page'] = source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning HTML pages**\n",
    "- since pages contain lot of information we don't need, we have to find tags, which contain reviews and descriptions, deduplicate and clean them from html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression to find tag which includes text for descriptions and reviews\n",
    "regexp = r'<span id=\\\"freeText.*'\n",
    "reviews = list()\n",
    "descriptions = list()\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    p = data.at[index,'page']\n",
    "    \n",
    "    #find tags including descriptions and reviews\n",
    "    texts = re.findall(regexp, p) \n",
    "    res = list()\n",
    "    \n",
    "    #remove html tags\n",
    "    for t in texts:\n",
    "        soup = BeautifulSoup(t, 'lxml')\n",
    "        res.append(soup.text)\n",
    "    \n",
    "    #remove duplicates (each longer text has short version and full version)\n",
    "    fin = list()\n",
    "    l = len(res) -1\n",
    "    last = 0\n",
    "    for i, x in enumerate(res):\n",
    "        if i < l:\n",
    "            if res[i][:50] == res[i+1][:50] and last != i:\n",
    "                fin.append(res[i+1])\n",
    "                last = i+1\n",
    "            elif res[i][:50] != res[i+1][:50] and last != i:\n",
    "                fin.append(res[i])\n",
    "                last = i    \n",
    "    \n",
    "    if len(fin) == 0:\n",
    "        continue\n",
    "    d= fin[0]\n",
    "    r = [x for b, x in enumerate(fin) if b>0]\n",
    "    \n",
    "    #save results\n",
    "    data.at[index,'description'] = d\n",
    "    data.at[index,'reviews'] = r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
